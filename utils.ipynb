{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8a7660-0d9c-48b0-9e52-e4e7968df2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from collections import defaultdict\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "lora_module_dict = {\n",
    "    'chatglm2': ['query_key_value'],\n",
    "    'llama2': [\n",
    "        'q_proj', 'k_proj', 'v_proj',\n",
    "        'o_proj', 'gate_proj', 'up_proj', 'down_proj',\n",
    "    ],\n",
    "    'deepseekR1': [\"q_proj\", \"v_proj\", \"gate_proj\"]\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize(args, tokenizer, feature):\n",
    "    \n",
    "    prompt_ids = tokenizer.encode(\n",
    "        feature['prompt'].strip(), padding=False,\n",
    "        max_length=args.max_length, truncation=True\n",
    "    )\n",
    "    \n",
    "    target_ids = tokenizer.encode(\n",
    "        feature['answer'].strip(), padding=False,\n",
    "        max_length=args.max_length, truncation=True, add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    input_ids = prompt_ids + target_ids\n",
    "    exceed_max_length = len(input_ids) >= args.max_length\n",
    "    \n",
    "     # Add EOS Token\n",
    "    if input_ids[-1] != tokenizer.eos_token_id and not exceed_max_length:\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "    \n",
    "    label_ids = [tokenizer.pad_token_id] * len(prompt_ids) + input_ids[len(prompt_ids):]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": label_ids,\n",
    "        \"exceed_max_length\": exceed_max_length\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_model_name(name, from_remote=False):\n",
    "    \n",
    "    if name == 'chatglm2':\n",
    "        return 'THUDM/chatglm2-6b' if from_remote else 'base_models/chatglm2-6b'\n",
    "    elif name == 'llama2':\n",
    "        return 'meta-llama/Llama-2-7b-chat-hf' # if from_remote else 'base_models/Llama-2-7b-chat-hf'\n",
    "    elif name == \"llama3\":\n",
    "      \t return \"meta-llama/Llama-3.1-8B\"\n",
    "    elif name == \"deepseekR1\":\n",
    "    \t  return \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "    else:\n",
    "        raise ValueError(f\"Undefined base model {name}\")\n",
    "        \n",
    "    \n",
    "def load_dataset(names, from_remote=False):\n",
    "    \n",
    "    dataset_names = [d for d in names.split(',')]\n",
    "    dataset_list = []\n",
    "    \n",
    "    for name in dataset_names:\n",
    "        rep = 1\n",
    "        if not os.path.exists(name):\n",
    "            rep = int(name.split('*')[1]) if '*' in name else 1\n",
    "            name = ('FinGPT/fingpt-forecaster-' if from_remote else 'data/fingpt-forecaster-') + name.split('*')[0]\n",
    "        tmp_dataset = datasets.load_dataset(name) if from_remote else datasets.load_from_disk(name)\n",
    "    \n",
    "        if 'test' not in tmp_dataset:\n",
    "            tmp_dataset = tmp_dataset.train_test_split(0.2, shuffle=True, seed=42)   \n",
    "        dataset_list.extend([tmp_dataset] * rep)\n",
    "    \n",
    "    return dataset_list\n",
    "\n",
    "\n",
    "def parse_answer(answer):\n",
    "    \n",
    "    match_res = re.match(r\"^\\s*\\[Positive Developments\\]:\\s*(.*)\\s*\\[Potential Concerns\\]:\\s*(.*)\\s*\\[Prediction (&|and) Analysis\\]:\\s*(.*)\\s*$\", answer, flags=re.DOTALL)\n",
    "    if not match_res:\n",
    "        return None\n",
    "    \n",
    "    pros, cons, pna = match_res.group(1), match_res.group(2), match_res.group(4)\n",
    "        \n",
    "    match_res = re.match(r'^Prediction:\\s*(.*)\\s*Analysis:\\s*(.*)\\s*$', pna, flags=re.DOTALL)\n",
    "    if not match_res:\n",
    "        return None\n",
    "        \n",
    "    pred, anal = match_res.group(1), match_res.group(2)\n",
    "        \n",
    "    if re.search(r'up|increase', pred.lower()):\n",
    "        pred_bin = 1\n",
    "    elif re.search(r'down|decrease|decline', pred.lower()):\n",
    "        pred_bin = -1\n",
    "    else:\n",
    "        pred_bin = 0\n",
    "            \n",
    "    match_res = re.search(r'(\\d)-(\\d)%', pred)\n",
    "    if not match_res:\n",
    "        match_res = re.search(r'(?:more than )?(\\d)+?%', pred)    \n",
    "        \n",
    "    pred_margin = pred_bin * (int(match_res.group(1)) + 0.5) if match_res else 0.\n",
    "        \n",
    "    return {\n",
    "        \"positive developments\": pros,\n",
    "        \"potential concerns\": cons,\n",
    "        \"prediction\": pred_margin,\n",
    "        \"prediction_binary\": pred_bin,\n",
    "        \"analysis\": anal\n",
    "    }\n",
    "    \n",
    "\n",
    "def calc_rouge_score(references, answers):\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "    scores_per_pair = [scorer.score(ref, ans) for ref, ans in zip(references, answers)]\n",
    "    \n",
    "    rouge1 = sum(score['rouge1'].fmeasure for score in scores_per_pair) / len(scores_per_pair)\n",
    "    rouge2 = sum(score['rouge2'].fmeasure for score in scores_per_pair) / len(scores_per_pair)\n",
    "    rougeL = sum(score['rougeL'].fmeasure for score in scores_per_pair) / len(scores_per_pair)\n",
    "    \n",
    "    return {'rouge1': rouge1, 'rouge2': rouge2, 'rougeL': rougeL}\n",
    "\n",
    "    \n",
    "def calc_metrics(answers, gts):\n",
    "    \n",
    "    answers_dict = defaultdict(list)\n",
    "    gts_dict = defaultdict(list)\n",
    "    \n",
    "    for answer, gt in zip(answers, gts):\n",
    "        answer_dict = parse_answer(answer)\n",
    "        gt_dict = parse_answer(gt)\n",
    "        \n",
    "        if answer_dict and gt_dict:\n",
    "            for k in answer_dict.keys():\n",
    "                answers_dict[k].append(answer_dict[k])\n",
    "                gts_dict[k].append(gt_dict[k])\n",
    "    \n",
    "    if not answers_dict['prediction']:\n",
    "        return {}\n",
    "    \n",
    "    bin_acc = accuracy_score(gts_dict['prediction_binary'], answers_dict['prediction_binary'])\n",
    "    mse = mean_squared_error(gts_dict['prediction'], answers_dict['prediction'])\n",
    "    \n",
    "    pros_rouge_scores = calc_rouge_score(gts_dict['positive developments'], answers_dict['positive developments'])\n",
    "    cons_rouge_scores = calc_rouge_score(gts_dict['potential concerns'], answers_dict['potential concerns'])\n",
    "    anal_rouge_scores = calc_rouge_score(gts_dict['analysis'], answers_dict['analysis'])\n",
    "                              \n",
    "    print(f\"\\nBinary Accuracy: {bin_acc:.2f}  |  Mean Square Error: {mse:.2f}\")\n",
    "    print(f\"\\nRouge Score of Positive Developments: {pros_rouge_scores}\")\n",
    "    print(f\"\\nRouge Score of Potential Concerns: {cons_rouge_scores}\")\n",
    "    print(f\"\\nRouge Score of Summary Analysis: {anal_rouge_scores}\")\n",
    "                              \n",
    "    return {\n",
    "        \"valid_count\": len(answers_dict['prediction']),\n",
    "        \"bin_acc\": bin_acc,\n",
    "        \"mse\": mse,\n",
    "        \"pros_rouge_scores\": pros_rouge_scores,\n",
    "        \"cons_rouge_scores\": cons_rouge_scores,\n",
    "        \"anal_rouge_scores\": anal_rouge_scores\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4142a0-397b-4ad8-a30e-5c341a778e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621de1e-44f3-48b7-98a8-c9d3beb1f2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
